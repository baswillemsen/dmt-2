{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started loading data from file C:\\Users\\Bas\\OneDrive\\MSc. Artificial Intelligence VU\\MSc. AI Year 1\\Data Mining Techniques\\Assignment 2\\data\\training_set_VU_DM.csv\n",
      "Finished loading data....\n",
      "Preprocessing training data....\n",
      "Dropping columns ['comp1_rate', 'comp1_inv', 'comp1_rate_percent_diff', 'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv', 'comp4_rate_percent_diff', 'comp6_rate', 'comp6_inv', 'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv', 'comp7_rate_percent_diff', 'gross_bookings_usd'] because they miss more than 0.9 of data.\n",
      "Dropped columns ['comp1_rate', 'comp1_inv', 'comp1_rate_percent_diff', 'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv', 'comp4_rate_percent_diff', 'comp6_rate', 'comp6_inv', 'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv', 'comp7_rate_percent_diff', 'gross_bookings_usd']\n",
      "   srch_id  site_id  visitor_location_country_id  visitor_hist_starrating  \\\n",
      "0    10053        5                          219                      NaN   \n",
      "1    10053        5                          219                      NaN   \n",
      "2    10053        5                          219                      NaN   \n",
      "3    10053        5                          219                      NaN   \n",
      "4    10053        5                          219                      NaN   \n",
      "\n",
      "   visitor_hist_adr_usd  prop_country_id  prop_id  prop_starrating  \\\n",
      "0                   NaN              219    21018                4   \n",
      "1                   NaN              219   117294                5   \n",
      "2                   NaN              219    59537                2   \n",
      "3                   NaN              219    54004                3   \n",
      "4                   NaN              219    38124                2   \n",
      "\n",
      "   prop_review_score  prop_brand_bool  ...  \\\n",
      "0                4.0                1  ...   \n",
      "1                5.0                1  ...   \n",
      "2                4.5                1  ...   \n",
      "3                4.5                1  ...   \n",
      "4                3.0                1  ...   \n",
      "\n",
      "   SUBSTRACT(prop_location_score1, MEAN)  \\\n",
      "0                               0.907879   \n",
      "1                               0.847879   \n",
      "2                              -0.322121   \n",
      "3                              -0.122121   \n",
      "4                              -0.732121   \n",
      "\n",
      "   MEAN(srch_destination_id, price_usd)  MEAN(srch_id, prop_review_score)  \\\n",
      "0                              2.009985                               4.0   \n",
      "1                              2.009985                               4.0   \n",
      "2                              2.009985                               4.0   \n",
      "3                              2.009985                               4.0   \n",
      "4                              2.009985                               4.0   \n",
      "\n",
      "   SUBSTRACT(prop_review_score, MEAN)  MEAN(srch_id, promotion_flag)  \\\n",
      "0                                 0.0                       0.272727   \n",
      "1                                 1.0                       0.272727   \n",
      "2                                 0.5                       0.272727   \n",
      "3                                 0.5                       0.272727   \n",
      "4                                -1.0                       0.272727   \n",
      "\n",
      "   SUBSTRACT(promotion_flag, MEAN)  prop_location_score2_norm_by_srch_id  \\\n",
      "0                         0.727273                                   NaN   \n",
      "1                        -0.272727                                   NaN   \n",
      "2                        -0.272727                             -0.397125   \n",
      "3                        -0.272727                                   NaN   \n",
      "4                        -0.272727                              0.353055   \n",
      "\n",
      "   prop_location_score1_norm_by_srch_id  prop_review_score_norm_by_srch_id  \\\n",
      "0                              0.741001                           0.000000   \n",
      "1                              0.692030                           0.992278   \n",
      "2                             -0.262912                           0.496139   \n",
      "3                             -0.099674                           0.496139   \n",
      "4                             -0.597549                          -0.992278   \n",
      "\n",
      "   estimated_position  \n",
      "0            0.049401  \n",
      "1            0.050967  \n",
      "2            0.137339  \n",
      "3            0.042135  \n",
      "4            0.106707  \n",
      "\n",
      "[5 rows x 60 columns]\n",
      "   srch_id  site_id  visitor_location_country_id  visitor_hist_starrating  \\\n",
      "0        1       12                          187                      NaN   \n",
      "1        1       12                          187                      NaN   \n",
      "2        1       12                          187                      NaN   \n",
      "3        1       12                          187                      NaN   \n",
      "4        1       12                          187                      NaN   \n",
      "\n",
      "   visitor_hist_adr_usd  prop_country_id  prop_id  prop_starrating  \\\n",
      "0                   NaN              219      893                3   \n",
      "1                   NaN              219   122844                3   \n",
      "2                   NaN              219   114766                2   \n",
      "3                   NaN              219   111106                3   \n",
      "4                   NaN              219   111000                3   \n",
      "\n",
      "   prop_review_score  prop_brand_bool  ...  \\\n",
      "0                3.5                1  ...   \n",
      "1                4.5                1  ...   \n",
      "2                3.5                1  ...   \n",
      "3                2.5                1  ...   \n",
      "4                4.5                1  ...   \n",
      "\n",
      "   SUBSTRACT(prop_location_score1, MEAN)  \\\n",
      "0                               0.530357   \n",
      "1                               0.000357   \n",
      "2                              -0.689643   \n",
      "3                              -1.609643   \n",
      "4                              -0.099643   \n",
      "\n",
      "   MEAN(srch_destination_id, price_usd)  MEAN(srch_id, prop_review_score)  \\\n",
      "0                              2.086554                          3.482143   \n",
      "1                              2.086554                          3.482143   \n",
      "2                              2.086554                          3.482143   \n",
      "3                              2.086554                          3.482143   \n",
      "4                              2.086554                          3.482143   \n",
      "\n",
      "   SUBSTRACT(prop_review_score, MEAN)  MEAN(srch_id, promotion_flag)  \\\n",
      "0                            0.017857                       0.035714   \n",
      "1                            1.017857                       0.035714   \n",
      "2                            0.017857                       0.035714   \n",
      "3                           -0.982143                       0.035714   \n",
      "4                            1.017857                       0.035714   \n",
      "\n",
      "   SUBSTRACT(promotion_flag, MEAN)  prop_location_score2_norm_by_srch_id  \\\n",
      "0                        -0.035714                             -0.109967   \n",
      "1                        -0.035714                             -0.969828   \n",
      "2                        -0.035714                             -0.927470   \n",
      "3                        -0.035714                             -0.887230   \n",
      "4                        -0.035714                             -0.690267   \n",
      "\n",
      "   prop_location_score1_norm_by_srch_id  prop_review_score_norm_by_srch_id  \\\n",
      "0                              1.022407                           0.016094   \n",
      "1                              0.000688                           0.917342   \n",
      "2                             -1.329473                           0.016094   \n",
      "3                             -3.103021                          -0.885154   \n",
      "4                             -0.192089                           0.917342   \n",
      "\n",
      "   estimated_position  \n",
      "0            0.037859  \n",
      "1            0.033208  \n",
      "2            0.032661  \n",
      "3            0.034001  \n",
      "4            0.049909  \n",
      "\n",
      "[5 rows x 60 columns]\n",
      "Training on train set with columns: ['site_id' 'visitor_location_country_id' 'visitor_hist_starrating'\n",
      " 'visitor_hist_adr_usd' 'prop_country_id' 'prop_starrating'\n",
      " 'prop_review_score' 'prop_brand_bool' 'prop_location_score1'\n",
      " 'prop_location_score2' 'prop_log_historical_price' 'price_usd'\n",
      " 'promotion_flag' 'srch_destination_id' 'srch_length_of_stay'\n",
      " 'srch_booking_window' 'srch_adults_count' 'srch_children_count'\n",
      " 'srch_room_count' 'srch_saturday_night_bool' 'srch_query_affinity_score'\n",
      " 'orig_destination_distance' 'comp2_rate' 'comp2_inv'\n",
      " 'comp2_rate_percent_diff' 'comp3_rate' 'comp3_inv' 'comp5_rate'\n",
      " 'comp5_inv' 'comp5_rate_percent_diff' 'comp8_rate' 'comp8_inv'\n",
      " 'comp8_rate_percent_diff' 'month' 'hour' 'dayofweek'\n",
      " 'price_usd_norm_by_srch_id' 'price_usd_norm_by_prop_id'\n",
      " 'prop_starrating_norm_by_srch_id' 'MEAN(prop_id, price_usd)'\n",
      " 'SUBSTRACT(price_usd, MEAN)' 'MEAN(srch_id, prop_starrating)'\n",
      " 'SUBSTRACT(prop_starrating, MEAN)' 'MEAN(srch_id, prop_location_score2)'\n",
      " 'SUBSTRACT(prop_location_score2, MEAN)'\n",
      " 'MEAN(srch_id, prop_location_score1)'\n",
      " 'SUBSTRACT(prop_location_score1, MEAN)'\n",
      " 'MEAN(srch_destination_id, price_usd)' 'MEAN(srch_id, prop_review_score)'\n",
      " 'SUBSTRACT(prop_review_score, MEAN)' 'MEAN(srch_id, promotion_flag)'\n",
      " 'SUBSTRACT(promotion_flag, MEAN)' 'prop_location_score2_norm_by_srch_id'\n",
      " 'prop_location_score1_norm_by_srch_id'\n",
      " 'prop_review_score_norm_by_srch_id' 'estimated_position']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [0, 1, 4, 33]\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Do not support special JSON characters in feature name.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bc25503b835b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[0mtest_csv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"C:\\Users\\Bas\\OneDrive\\MSc. Artificial Intelligence VU\\MSc. AI Year 1\\Data Mining Techniques\\Assignment 2\\data\\test_set_VU_DM.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"C:\\Users\\Bas\\OneDrive\\MSc. Artificial Intelligence VU\\MSc. AI Year 1\\Data Mining Techniques\\Assignment 2\\personalize_expedia_hotel_searches_2013\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_csv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_csv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-bc25503b835b>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(train_csv, test_csv, output_dir)\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m         )\n\u001b[1;32m--> 432\u001b[1;33m         model = train_model(\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_groups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_of_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m         )\n",
      "\u001b[1;32m<ipython-input-2-bc25503b835b>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(x1, x2, y1, y2, groups, eval_groups, lr, method, output_dir, name_of_model)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training on train set with columns: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m     clf.fit(\n\u001b[0m\u001b[0;32m    368\u001b[0m         \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_group, eval_metric, eval_at, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eval_at\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_at\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1067\u001b[1;33m         super().fit(X, y, sample_weight=sample_weight, init_score=init_score, group=group,\n\u001b[0m\u001b[0;32m   1068\u001b[0m                     \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_sample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m                     \u001b[0meval_init_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_init_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_group\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_group\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    746\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m         self._Booster = train(\n\u001b[0m\u001b[0;32m    749\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;31m# construct booster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m         \u001b[0mbooster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[0;32m   2603\u001b[0m                 )\n\u001b[0;32m   2604\u001b[0m             \u001b[1;31m# construct booster object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2605\u001b[1;33m             \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2606\u001b[0m             \u001b[1;31m# copy the parameters from train_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2607\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1813\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1814\u001b[0m                 \u001b[1;31m# create train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1815\u001b[1;33m                 self._lazy_init(self.data, label=self.label,\n\u001b[0m\u001b[0;32m   1816\u001b[0m                                 \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1817\u001b[0m                                 \u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m   1571\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Wrong predictor type {type(predictor).__name__}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1572\u001b[0m         \u001b[1;31m# set feature names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1573\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_feature_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1575\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mset_feature_name\u001b[1;34m(self, feature_name)\u001b[0m\n\u001b[0;32m   2140\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Length of feature_name({len(feature_name)}) and num_feature({self.num_feature()}) don't match\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2141\u001b[0m             \u001b[0mc_feature_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2142\u001b[1;33m             _safe_call(_LIB.LGBM_DatasetSetFeatureNames(\n\u001b[0m\u001b[0;32m   2143\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2144\u001b[0m                 \u001b[0mc_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_feature_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \"\"\"\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLightGBMError\u001b[0m: Do not support special JSON characters in feature name."
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This is the competition:\n",
    "https://www.kaggle.com/c/expedia-personalized-sort/discussion\n",
    "\n",
    "Evaluation NDCG@5\n",
    "points:\n",
    "\n",
    "5 - The user purchased a room at this hotel\n",
    "\n",
    "1 - The user clicked through to see more information on this hotel\n",
    "\n",
    "0 - The user neither clicked on this hotel nor purchased a room at this hotel\n",
    "\"\"\"\n",
    "import time\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import re\n",
    "import pandas\n",
    "import numpy as np\n",
    "import lightgbm\n",
    "\n",
    "def load_data(file_path):\n",
    "    gc.collect()\n",
    "    print(\"Started loading data from file {}\".format(file_path))\n",
    "    orig_data = pandas.read_csv(file_path)\n",
    "    orig_data = orig_data.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    print(\"Finished loading data....\")\n",
    "    return orig_data\n",
    "\n",
    "\n",
    "def add_date_features(\n",
    "    in_data, datetime_key=\"date_time\", features=[\"month\", \"hour\", \"dayofweek\"]\n",
    "):\n",
    "    dates = pandas.to_datetime(in_data[datetime_key])\n",
    "    for feature in features:\n",
    "        if feature == \"month\":\n",
    "            in_data[\"month\"] = dates.dt.month\n",
    "        elif feature == \"dayofweek\":\n",
    "            in_data[\"dayofweek\"] = dates.dt.dayofweek\n",
    "        elif feature == \"hour\":\n",
    "            in_data[\"hour\"] = dates.dt.hour\n",
    "\n",
    "    return in_data\n",
    "\n",
    "\n",
    "def normalize_features(input_df, group_key, target_column, take_log10=False):\n",
    "\n",
    "    # for numerical stability\n",
    "    epsilon = 1e-4\n",
    "    if take_log10:\n",
    "        input_df[target_column] = np.log10(input_df[target_column] + epsilon)\n",
    "    methods = [\"mean\", \"std\"]\n",
    "\n",
    "    df = input_df.groupby(group_key).agg({target_column: methods})\n",
    "\n",
    "    df.columns = df.columns.droplevel()\n",
    "    col = {}\n",
    "    for method in methods:\n",
    "        col[method] = target_column + \"_\" + method\n",
    "\n",
    "    df.rename(columns=col, inplace=True)\n",
    "    df_merge = input_df.merge(df.reset_index(), on=group_key)\n",
    "    df_merge[target_column + \"_norm_by_\" + group_key] = (\n",
    "        df_merge[target_column] - df_merge[target_column + \"_mean\"]\n",
    "    ) / df_merge[target_column + \"_std\"]\n",
    "    df_merge = df_merge.drop(labels=[col[\"mean\"], col[\"std\"]], axis=1)\n",
    "\n",
    "    gc.collect()\n",
    "    return df_merge\n",
    "\n",
    "\n",
    "# Add new columns to the dataframe\n",
    "def aggregated_features_single_column(\n",
    "    in_data,\n",
    "    key_for_grouped_by=\"prop_id\",\n",
    "    target_column=\"price_usd\",\n",
    "    agg_methods=[\"mean\", \"median\", \"min\", \"max\"],\n",
    "    transform_methods={\"mean\": [\"substract\"]},\n",
    "):\n",
    "    df = in_data.groupby(key_for_grouped_by).agg({target_column: agg_methods})\n",
    "\n",
    "    if isinstance(key_for_grouped_by, list):\n",
    "        str_key_for_grouped_by = \"|\".join(key_for_grouped_by)\n",
    "    else:\n",
    "        str_key_for_grouped_by = key_for_grouped_by\n",
    "\n",
    "    df.columns = df.columns.droplevel()\n",
    "    col = {}\n",
    "    for method in agg_methods:\n",
    "        col[method] = (\n",
    "            method.upper() + \"(\" + str_key_for_grouped_by + \", \" + target_column + \")\"\n",
    "        )\n",
    "\n",
    "    df.rename(columns=col, inplace=True)\n",
    "\n",
    "    in_data = in_data.merge(df.reset_index(), on=key_for_grouped_by)\n",
    "    for method_name in transform_methods:\n",
    "        for applying_function in transform_methods[method_name]:\n",
    "            function_data = in_data[\n",
    "                method_name.upper()\n",
    "                + \"(\"\n",
    "                + str_key_for_grouped_by\n",
    "                + \", \"\n",
    "                + target_column\n",
    "                + \")\"\n",
    "            ]\n",
    "            column_data = in_data[target_column]\n",
    "            if applying_function == \"substract\":\n",
    "                result = column_data - function_data\n",
    "            elif applying_function == \"divide\":\n",
    "                result = column_data / function_data\n",
    "            else:\n",
    "                continue\n",
    "            in_data[\n",
    "                applying_function.upper()\n",
    "                + \"(\"\n",
    "                + target_column\n",
    "                + \", \"\n",
    "                + method_name.upper()\n",
    "                + \")\"\n",
    "            ] = result\n",
    "    gc.collect()\n",
    "\n",
    "    return in_data\n",
    "\n",
    "\n",
    "def drop_columns_with_missing_data(\n",
    "    df,\n",
    "    threshold,\n",
    "    ignore_values=[\n",
    "        \"visitor_hist_adr_usd\",\n",
    "        \"visitor_hist_starrating\",\n",
    "        \"srch_query_affinity_score\",\n",
    "    ],\n",
    "):\n",
    "    columns_to_drop = []\n",
    "\n",
    "    for i in range(df.shape[1]):\n",
    "        length_df = len(df)\n",
    "        column_names = df.columns.tolist()\n",
    "        number_nans = sum(df.iloc[:, i].isnull())\n",
    "        if number_nans / length_df > threshold:\n",
    "            if column_names[i] not in ignore_values:\n",
    "                columns_to_drop.append(column_names[i])\n",
    "\n",
    "    print(\n",
    "        \"Dropping columns {} because they miss more than {} of data.\".format(\n",
    "            columns_to_drop, threshold\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_reduced = df.drop(labels=columns_to_drop, axis=1)\n",
    "    print(\"Dropped columns {}\".format(columns_to_drop))\n",
    "    return df_reduced\n",
    "\n",
    "\n",
    "def preprocess_training_data(orig_data, kind=\"train\", use_ndcg_choices=False):\n",
    "\n",
    "    print(\"Preprocessing training data....\")\n",
    "    gc.collect()\n",
    "    data_for_training = orig_data\n",
    "\n",
    "    target_column = \"target\"\n",
    "\n",
    "    if kind == \"train\":\n",
    "        conditions = [\n",
    "            data_for_training[\"click_bool\"] == 1,\n",
    "            data_for_training[\"booking_bool\"] == 1,\n",
    "        ]\n",
    "        choices = [1, 2]\n",
    "        data_for_training[target_column] = np.select(conditions, choices, default=0)\n",
    "\n",
    "    threshold = 0.9\n",
    "    data_for_training = add_date_features(data_for_training)\n",
    "    data_for_training.drop(labels=[\"date_time\"], axis=1, inplace=True)\n",
    "\n",
    "    data_for_training = drop_columns_with_missing_data(data_for_training, threshold)\n",
    "\n",
    "    # do not normalize 2 times with take_log10\n",
    "    data_for_training = normalize_features(\n",
    "        data_for_training,\n",
    "        group_key=\"srch_id\",\n",
    "        target_column=\"price_usd\",\n",
    "        take_log10=True,\n",
    "    )\n",
    "    data_for_training = normalize_features(\n",
    "        data_for_training, group_key=\"prop_id\", target_column=\"price_usd\"\n",
    "    )\n",
    "    data_for_training = normalize_features(\n",
    "        data_for_training, group_key=\"srch_id\", target_column=\"prop_starrating\"\n",
    "    )\n",
    "\n",
    "    data_for_training = aggregated_features_single_column(\n",
    "        data_for_training, \"prop_id\", \"price_usd\", [\"mean\"]\n",
    "    )\n",
    "    data_for_training = aggregated_features_single_column(\n",
    "        data_for_training,\n",
    "        key_for_grouped_by=\"srch_id\",\n",
    "        target_column=\"prop_starrating\",\n",
    "        agg_methods=[\"mean\"],\n",
    "        transform_methods={\"mean\": [\"substract\"]},\n",
    "    )\n",
    "    data_for_training = aggregated_features_single_column(\n",
    "        data_for_training,\n",
    "        key_for_grouped_by=\"srch_id\",\n",
    "        target_column=\"prop_location_score2\",\n",
    "        agg_methods=[\"mean\"],\n",
    "        transform_methods={\"mean\": [\"substract\"]},\n",
    "    )\n",
    "    data_for_training = aggregated_features_single_column(\n",
    "        data_for_training,\n",
    "        key_for_grouped_by=\"srch_id\",\n",
    "        target_column=\"prop_location_score1\",\n",
    "        agg_methods=[\"mean\"],\n",
    "        transform_methods={\"mean\": [\"substract\"]},\n",
    "    )\n",
    "    data_for_training = aggregated_features_single_column(\n",
    "        data_for_training,\n",
    "        key_for_grouped_by=\"srch_destination_id\",\n",
    "        target_column=\"price_usd\",\n",
    "        agg_methods=[\"mean\"],\n",
    "        transform_methods={\"mean\": [\"substract\"]},\n",
    "    )\n",
    "    data_for_training = aggregated_features_single_column(\n",
    "        data_for_training,\n",
    "        key_for_grouped_by=\"srch_id\",\n",
    "        target_column=\"prop_review_score\",\n",
    "        agg_methods=[\"mean\"],\n",
    "        transform_methods={\"mean\": [\"substract\"]},\n",
    "    )\n",
    "    data_for_training = aggregated_features_single_column(\n",
    "        data_for_training,\n",
    "        key_for_grouped_by=\"srch_id\",\n",
    "        target_column=\"promotion_flag\",\n",
    "        agg_methods=[\"mean\"],\n",
    "        transform_methods={\"mean\": [\"substract\"]},\n",
    "    )\n",
    "\n",
    "    # NOTE: has to be done after aggregated_features_single_column\n",
    "    data_for_training = data_for_training.sort_values(\"srch_id\")\n",
    "\n",
    "    data_for_training = normalize_features(\n",
    "        data_for_training, group_key=\"srch_id\", target_column=\"prop_starrating\"\n",
    "    )\n",
    "    data_for_training = normalize_features(\n",
    "        data_for_training, group_key=\"srch_id\", target_column=\"prop_location_score2\"\n",
    "    )\n",
    "    data_for_training = normalize_features(\n",
    "        data_for_training, group_key=\"srch_id\", target_column=\"prop_location_score1\"\n",
    "    )\n",
    "    data_for_training = normalize_features(\n",
    "        data_for_training, group_key=\"srch_id\", target_column=\"prop_review_score\"\n",
    "    )\n",
    "\n",
    "    gc.collect()\n",
    "    if kind == \"train\":\n",
    "        y = data_for_training[target_column].values\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    training_set_only_metrics = [\"click_bool\", \"booking_bool\", \"gross_bookings_usd\"]\n",
    "    columns_to_remove = [\n",
    "        \"date_time\",\n",
    "        \"target\",\n",
    "        target_column,\n",
    "    ] + training_set_only_metrics\n",
    "    columns_to_remove = [\n",
    "        c for c in columns_to_remove if c in data_for_training.columns.values\n",
    "    ]\n",
    "    data_for_training = data_for_training.drop(labels=columns_to_remove, axis=1)\n",
    "    return data_for_training, y\n",
    "\n",
    "\n",
    "def remove_columns(x1, ignore_column=[\"srch_id\", \"prop_id\", \"position\", \"random_bool\"]):\n",
    "    ignore_column = [c for c in ignore_column if c in x1.columns.values]\n",
    "    # print('Dropping columns: {}'.format(ignore_column))\n",
    "    # ignore_column_numbers = [x1.columns.get_loc(x) for x in ignore_column]\n",
    "    x1 = x1.drop(labels=ignore_column, axis=1)\n",
    "    # print('Columns after dropping: {}'.format(x1.columns.values))\n",
    "    return x1\n",
    "\n",
    "\n",
    "def input_estimated_position(training_data, srch_id_dest_id_dict):\n",
    "    training_data = training_data.merge(\n",
    "        srch_id_dest_id_dict, how=\"left\", on=[\"srch_destination_id\", \"prop_id\"]\n",
    "    )\n",
    "    print(training_data.head())\n",
    "    return training_data\n",
    "\n",
    "\n",
    "def split_train_data(data_for_training, y, val_start=0, val_end=0):\n",
    "\n",
    "    x1 = pandas.concat([data_for_training[0:val_start], data_for_training[val_end:]])\n",
    "    y1 = np.concatenate((y[0:val_start], y[val_end:]), axis=0)\n",
    "    x2 = data_for_training[val_start:val_end]\n",
    "    y2 = y[val_start:val_end]\n",
    "\n",
    "    srch_id_dest_id_dict = x1.loc[x1[\"random_bool\"] == 0]\n",
    "\n",
    "    # estimated position calculation\n",
    "    srch_id_dest_id_dict = x1.loc[x1[\"random_bool\"] == 0]\n",
    "    srch_id_dest_id_dict = x1.groupby([\"srch_destination_id\", \"prop_id\"]).agg(\n",
    "        {\"position\": \"mean\"}\n",
    "    )\n",
    "    srch_id_dest_id_dict = srch_id_dest_id_dict.rename(\n",
    "        index=str, columns={\"position\": \"estimated_position\"}\n",
    "    ).reset_index()\n",
    "    srch_id_dest_id_dict[\"srch_destination_id\"] = (\n",
    "        srch_id_dest_id_dict[\"srch_destination_id\"].astype(str).astype(int)\n",
    "    )\n",
    "    srch_id_dest_id_dict[\"prop_id\"] = (\n",
    "        srch_id_dest_id_dict[\"prop_id\"].astype(str).astype(int)\n",
    "    )\n",
    "    srch_id_dest_id_dict[\"estimated_position\"] = (\n",
    "        1 / srch_id_dest_id_dict[\"estimated_position\"]\n",
    "    )\n",
    "    x1 = input_estimated_position(x1, srch_id_dest_id_dict)\n",
    "    x2 = input_estimated_position(x2, srch_id_dest_id_dict)\n",
    "\n",
    "    groups = x1[\"srch_id\"].value_counts(sort=False).sort_index()\n",
    "    eval_groups = x2[\"srch_id\"].value_counts(sort=False).sort_index()\n",
    "    len(eval_groups), len(x2), len(x1), len(groups)\n",
    "\n",
    "    x1 = remove_columns(x1)\n",
    "    x2 = remove_columns(x2)\n",
    "    return (x1, x2, y1, y2, groups, eval_groups, srch_id_dest_id_dict)\n",
    "\n",
    "\n",
    "def get_categorical_column(x1):\n",
    "    categorical_features = [\n",
    "        \"day\",\n",
    "        \"month\",\n",
    "        \"prop_country_id\",\n",
    "        \"site_id\",\n",
    "        \"visitor_location_country_id\",\n",
    "    ]\n",
    "    categorical_features = [c for c in categorical_features if c in x1.columns.values]\n",
    "    categorical_features_numbers = [x1.columns.get_loc(x) for x in categorical_features]\n",
    "    return categorical_features_numbers\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    x1, x2, y1, y2, groups, eval_groups, lr, method, output_dir, name_of_model=None\n",
    "):\n",
    "    if not name_of_model:\n",
    "        name_of_model = str(int(time.time()))\n",
    "\n",
    "    categorical_features_numbers = get_categorical_column(x1)\n",
    "    clf = lightgbm.LGBMRanker(\n",
    "        objective=\"lambdarank\",\n",
    "        metric=\"ndcg\",\n",
    "        n_estimators=5000,\n",
    "        learning_rate=lr,\n",
    "        max_position=5,\n",
    "        label_gain=[0, 1, 2],\n",
    "        random_state=69,\n",
    "        seed=69,\n",
    "        boosting=method,\n",
    "    )\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Training on train set with columns: {}\".format(x1.columns.values))\n",
    "    clf.fit(\n",
    "        x1,\n",
    "        y1,\n",
    "        eval_set=[(x1, y1), (x2, y2)],\n",
    "        eval_group=[groups, eval_groups],\n",
    "        group=groups,\n",
    "        eval_at=5,\n",
    "        verbose=20,\n",
    "        early_stopping_rounds=200,\n",
    "        categorical_feature=categorical_features_numbers,\n",
    "    )\n",
    "    gc.collect()\n",
    "    pickle.dump(clf, open(os.path.join(output_dir, \"model.dat\"), \"wb\"))\n",
    "    return clf\n",
    "\n",
    "\n",
    "def predict(name_of_model, test_data, srch_id_dest_id_dict, output_dir):\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    model = pickle.load(open(os.path.join(output_dir, \"model.dat\"), \"rb\"))\n",
    "\n",
    "    test_data = test_data.copy()\n",
    "    test_data = input_estimated_position(test_data, srch_id_dest_id_dict)\n",
    "\n",
    "    test_data_srch_id_prop_id = test_data[[\"srch_id\", \"prop_id\"]]\n",
    "\n",
    "    test_data = remove_columns(test_data)\n",
    "\n",
    "    categorical_features_numbers = get_categorical_column(test_data)\n",
    "\n",
    "    print(\"Predicting on train set with columns: {}\".format(test_data.columns.values))\n",
    "    kwargs = {}\n",
    "    kwargs = {\"categorical_feature\": categorical_features_numbers}\n",
    "\n",
    "    predictions = model.predict(test_data, **kwargs)\n",
    "    test_data_srch_id_prop_id[\"prediction\"] = predictions\n",
    "    del test_data\n",
    "    gc.collect()\n",
    "\n",
    "    test_data_srch_id_prop_id = test_data_srch_id_prop_id.sort_values(\n",
    "        [\"srch_id\", \"prediction\"], ascending=False\n",
    "    )\n",
    "    print(\"Saving predictions into submission.csv\")\n",
    "    test_data_srch_id_prop_id[[\"srch_id\", \"prop_id\"]].to_csv(\n",
    "        os.path.join(output_dir, \"submission.csv\"), index=False\n",
    "    )\n",
    "\n",
    "\n",
    "def run(train_csv, test_csv, output_dir):\n",
    "    name_of_model = str(int(time.time()))\n",
    "\n",
    "    training_data = load_data(train_csv)\n",
    "    training_data, y = preprocess_training_data(training_data)\n",
    "\n",
    "    method = \"dart\"\n",
    "    validation_num = 150000\n",
    "    lr = 0.12\n",
    "    # for i in range(0, int(len(training_data.index) / validation_num)): # enable for cross-validation\n",
    "    for i in range(0, 1):\n",
    "        val_start = i * validation_num\n",
    "        val_end = (i + 1) * validation_num\n",
    "        x1, x2, y1, y2, groups, eval_groups, srch_id_dest_id_dict = split_train_data(\n",
    "            training_data, y, val_start, val_end\n",
    "        )\n",
    "        model = train_model(\n",
    "            x1, x2, y1, y2, groups, eval_groups, lr, method, output_dir, name_of_model\n",
    "        )\n",
    "        test_data = load_data(test_csv)\n",
    "        test_data, _ = preprocess_training_data(test_data, kind=\"test\")\n",
    "        predict(name_of_model, test_data, srch_id_dest_id_dict, output_dir)\n",
    "        print(\"Submit the predictions file submission.csv to kaggle\")\n",
    "\n",
    "\n",
    "train_csv = r\"C:\\Users\\Bas\\OneDrive\\MSc. Artificial Intelligence VU\\MSc. AI Year 1\\Data Mining Techniques\\Assignment 2\\data\\training_set_VU_DM.csv\"\n",
    "test_csv = r\"C:\\Users\\Bas\\OneDrive\\MSc. Artificial Intelligence VU\\MSc. AI Year 1\\Data Mining Techniques\\Assignment 2\\data\\test_set_VU_DM.csv\"\n",
    "output_dir = r\"C:\\Users\\Bas\\OneDrive\\MSc. Artificial Intelligence VU\\MSc. AI Year 1\\Data Mining Techniques\\Assignment 2\\personalize_expedia_hotel_searches_2013\"\n",
    "run(train_csv, test_csv, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-0507e95749ce>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-0507e95749ce>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    pip3 install -r requirements.txt\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
